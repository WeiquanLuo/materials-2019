---
title: "Stat 585 - blog 3 - recap"
author: "Heike Hofmann"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

# Ethics of Reproducibility
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

1. not being able to reproduce results holds science back

--

  - best case scenario: published results are correct - but can't be reproduced because of accessibility issues 

  - worst case scenario: published results are incorrect - might steer the field in a wrong direction

  - either way: loss of resources spent (on trying) to reproduce results that could be spent on new discoveries

--

2. It is our **responsibility** to ensure that our research results are reproducible

--
3. The only choice we have in the matter is "how do we ensure reproducibility?"



---

# Ten simple rules

 by [Sandve et al](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285)


1. For Every Result, Keep Track of How It Was Produced

2. Avoid Manual Data Manipulation Steps

3. Archive the Exact Versions of All External Programs Used

4. Version Control All Custom Scripts

5. Record All Intermediate Results, When Possible in Standardized Formats

6. For Analyses That Include Randomness, Note Underlying Random Seeds

7. Always Store Raw Data behind Plots

8. Generate Hierarchical Analysis Output, Allowing Layers of Increasing Detail to Be Inspected

9. Connect Textual Statements to Underlying Results

10.  Provide Public Access to Scripts, Runs, and Results

---

## Generally

- ALL of these rules help with (computation) reproducibility

- When do we  follow rules? - when it's easy!

- Whenever things are (technically) difficult, or we have to be super disciplined, rules fall to the side

- Some rules are hard or even impossible to follow because of constraints outside  our control

---

## The hardest rule to follow

Responses from the class:

| Rule  	| # Responses 	|
|--------	|-------	|
| Rule 1	|  II       	|
| Rule 2	|  IIIII I     	|
| Rule 3	|  IIIII III     	|
| Rule 4	|  II     	|
| Rule 5	|  IIIII II     	|
| Rule 6 | |
| Rule 7 | I |
| Rule 8 | |
| Rule 9 | |
| Rule 10 | III |
---

## More specifically

Rule 1: **For Every Result, Keep Track of How It Was Produced**

can be quite daunting in collaborative efforts, but really is the foundation of any scientific result

RMarkdown helps to keep track of the connection between data, method and summary.

<br>
<br>
Rule 2: **Avoid Manual Data Manipulation Steps**

Technically difficult, manual steps are "unavoidable"? No!! Only use scripts for data manipulations.

---

Rule 3: **Archive the Exact Versions of All External Programs Used**

highest standard: a copy of the external program in the right version is available 

lower standard: document which versions of which programs were used

`sessionInfo()` helps with figuring out versions in R

Personally, I prefer `devtools::session_info()` - it provides a data frame of the same info in a more readable form

<br>
<br>

Rule 4: **Version Control All Custom Scripts**

high standard: anything generalizable (i.e. scripts you use between different projects) is part of a package (your own)

R package system offers version control

minimal standard: Include  the bits and pieces of scripts you create during development in your repository

---

Rule 5: **Record All Intermediate Results, When Possible in Standardized Formats**

Yes, it's hard, mostly because of the discipline it takes. 
It's easy to lose track of dependencies between files and hard to ensure consistency across

Pro: intermediate results provide duplication have saved a lot of projects when 'something catastrophic happened' 

one solution: RMarkdown supports automatic caching 

Technical issues: disk space


<br>
<br>
Rule 6. **For Analyses That Include Randomness, Note Underlying Random Seeds**

should be a no-brainer

in R: `set.seed()` and `varComp::get.seed()`

---

Rule 7: **Always Store Raw Data behind Plots**

Plots are intermediate results, so already covered by rule 5


<br>
<br>

Rule 8: **Generate Hierarchical Analysis Output, Allowing Layers of Increasing Detail to Be Inspected**

Rule 9: **Connect Textual Statements to Underlying Results**

Not strictly necessary for computational reproducibility, but giving context to results both in textual and quantitative form is good practice

Helps to debug when things go wrong


---

Rule 10: **Provide Public Access to Scripts, Runs, and Results**

This is what it is all about - making sure that other people can reproduce your results

Problems you mentioned: privacy restrictions on data, changes in software dependencies


<br>
<br>

## Summary

Making your results reprodcible is an extremely hard task

Often frustrating, because we know we will fail in some way

Let's just aim at getting better and checking a couple more of these boxes in the next project.

---

## Notes on grading

We deducted a couple of points from blog posts this time.

Mostly for a lack of depth in the discussion. You are expected to give reasons for statements that go beyond opinions and personal preferences.
