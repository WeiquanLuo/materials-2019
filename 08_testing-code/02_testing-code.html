<!DOCTYPE html>
<html>
  <head>
    <title>Stat 585 - Testing Your Code</title>
    <meta charset="utf-8">
    <meta name="author" content="Susan Vanderplas" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="tweaks.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Stat 585 - Testing Your Code
### Susan Vanderplas

---

class: center, middle



## Outline
.left[
- Motivation

- Automated Testing and Unit Tests

- Testing Functions in R with `testthat`

- Package Checks 

- Automatic Testing and Checking with TravisCI

- Test coverage with `covr`
]

???

This set of slides is going to cover a lot of ground - current best practices for testing to make sure your code does what you want it to do, how to automate the test process so that it isn't too tedious, setting your package up to automatically test itself with each new commit, and then how to make sure each line of your package is tested. It's a lot of ground to cover, so bear with me and ask lots of questions.

---

## So you have a package

Workflow:

1. Write a function.

2. Load it with `Ctrl/Cmd + Shift + L` or `devtools::load_all()`

3. Experiment with it in the console to see if it works.

4. Rinse and repeat.

&lt;br/&gt;&lt;br/&gt;

???
Typically, you write a function, you test it informally and experiment to make sure it's doing what you want it to do, and then you fix it, reload it, and test it again.

--

Testing in this workflow is informal, but can be repetitive and tedious. 


---
class:middle,center
# Automated Testing

???
There are more structured ways to do this same task, though, and it's helpful to use them when you're working on a large or complex problem. 
---

## Unit tests

Tests that evaluate one feature of a function



They are run automatically after you make changes to the code



Unit tests are one part of the larger framework of _Test Driven Development_

???

Unit tests are tests that evaluate a single unit of code (ideally, each function only does one thing, so a test of a function is a test of that single unit). In a testing framework, each time you make changes to the code, you run all of the tests to ensure that everything is still working. Unit tests are part of a larger philosophy called test driven development. 
---

## Unit tests

Tests that evaluate one feature of a function



They are run automatically after you make changes to the code



Unit tests are one part of the larger framework of _Test Driven Development_

## Test Driven Development

_Test Driven Development_ is an approach to programming where the unit tests are written before the actual code

- Requirements are decided in advance

- Code must meet the requirements (and only the requirements)

???

Test driven development is one philosophy of how to develop code. It's useful in its place, but I'm not going to say it's the only way to develop code. However, it's a nice way to think about things - you think first about what you want the function to do, then you write tests to ensure the function does that thing. Then, and only then, you write the function to satisfy those tests. The goal is to get a minimal set of code to fullfill the essential functions, and to make sure that every essential function is tested.

---

## Why use unit tests?

For a bit of extra work, you get: 

- Fewer bugs
    - Essential functions are tested and issues are caught quickly
    - Visual confirmation that essential features are working
    - When debugging, write a test to pinpoint the bug (and stop it from reoccurring)

- Better code structure
    - Modular code: do only one thing per function
    - Document functions as you write them to satisfy tests

- Robust code 
    - make big changes quickly and without downstream problems
    - Any changes that break things should be caught with tests

- (in theory) Easier to start after a break:    
pick up at the last failed test and write code to satisfy that test

???

The advantages of unit testing and test driven development are mostly from the extra structure of ensuring you write only the code you need. 
- If you're testing each function, you'll tend to make your functions smaller, so that they only do one task that can be tested. These modular functions are easier to work with later, too, because you can usually see exactly where something breaks. 
- You'll get fewer bugs, because you are testing each essential feature; when you do get a bug, you write a test for the bug so that it doesn't reappear later. 
- Your functions will be easier to document, because you wrote out ahead of time exactly what they do
- When you're interrupted, you write a test for the thing you were going to work on, then you pick up where the test failed and start writing code at that point.

You can use unit testing without buying into the whole test driven development framework, though - see how it works for you, and then you can get more serious about it later.
---
class:middle,center
# Testing in R: `testthat`

???
Unit testing in R is most commonly done with the `testthat` package. It integrates with RStudio nicely and makes it fairly easy to write tests for your R functions. As with the other functions today, we'll use the `usethis` package to set up `testthat`.

---

## Tools for package testing

1. `testthat`: unit testing for R packages
  
  - structured R package testing
  
  - provides functions to set up and tear down testing environments
  
  - runs each set of tests in a clean environment
  
  - reports whether each test passes or fails

2. `usethis`: helper functions for R package development

  - set up `testthat` for your package: `use_testthat()`
  
  - create new test files: `use_test("testname")` creates a new test file

.small[Note: `testthat` works with packages. [This github issue](https://github.com/r-lib/testthat/issues/659) has a good discussion of testing outside of the package framework]

???

The `testthat` package provides a structure for R package testing. You can set up a test environment, run your tests automatically in that clean environment, then tear the environment down. At the end of the test process, you get a report about which tests pass and fail.

The `usethis` package has some helper functions for R package development, including functions to set up your package to use `testthat`. If you want to create some new tests, you can use the `usethis` command `use_test("testname")` and it will create a file with the right structure in the right place.

---

## Package Testing Workflow

1. Modify your code or tests

2. Test your package (`Ctrl/CMD - Shift - T`)

3. Repeat until all tests pass

???

With `testthat`, the workflow becomes: modify your code or tests, run your tests, repeat until everything passes. It's more structured than the previous workflow, and you run the same tests each time, so you know that your function is working with the same tests - it's much harder to forget something you used to test!

---
class:inverse
## Your Turn


```r
## install.packages(c("testthat", "usethis"))
library(testthat)
```

1. Install the `testthat` package for unit testing and the `usethis` package (helper functions to make package development easy)

2. Set up a github repository for your new package (with a readme)

3. Create a new R package, `helloR`, and set it up to work with `testthat`

```r
# Run create_package to add package infrastructure 
# Adjust the path to refer to your current project directory
usethis::create_package("../helloR") 
usethis::use_testthat()
```

What does your file structure look like now? 

???
In this your turn, install usethis and testthat, and load them. 

You're going to make a new package and set it up to work with testthat. You'll need to set up a github repository for your package. Use RStudio to clone that github repository, and then use the `usethis` package to create a package structure. 

Once you've set that up, you're ready to set up `testthat`.

Your file structure: a Description and namespace file, an R folder with an R file, and a tests folder. Inside the tests folder is a testthat.R file and a testthat directory, which will contain your unit test files. 
---

## Testing Structure

A test file consists of 

1. context - a description of the test blocks in the file

2. one or more test blocks:    
`test_that(description, {test statements})`


```r
context("test-basics")

# Test block
test_that(
  # description
  "multiplication works",
  { # test statements inside this block
    expect_equal(2 * 2, 4)
  }
)
```
???

Now that our package is set up to use testthat, we have to figure out how to actually write unit tests. 

A test file has a context (a description of what you're testing inside the file) and then one or more test blocks. A test block consists of a test_that() statement, a description of what the test block is testing, and then an expression containing one or more expectations.

An expectation is a function that starts with expect_ and details a condition that should be met. In the statement here, testing whether multiplicaiton still works, we expect that 2*2 equals 4. 

`testthat` is pretty good about this - when testing equality, we have to consider machine precision. `expect_equal` uses `all.equal()` to test equality, so it isn't going to throw an error if there are tiny differences in the two numbers. 

A block of tests can have multiple expectations, so if you need to check that 0\*1 = 0 and -2\*2 = -4, you can do those tests in the same block.

---

## Test Statements

`testthat` has a series of expectation functions:

function | description
---- | ----
`expect_equal(obj, value)` | Is the object equal to a value?
`expect_error(expr)` | Does the expression produce an error?
`expect_gt(obj, value)` | Is the object greater than the value?
`expect_length(obj, value)` | Does the object have length value?

See more with `help(package = "testthat")`

These functions are silent if the expectation is met, and throw an error otherwise.

Expectations are used to construct tests. 
&lt;!-- When included in a test file, they will be run automatically; when run on their own, you can see how they work --&gt;
???
There are a bunch of different expectation functions in testthat. You can test for equality, whether something is greater than a value (or less than), that something has the correct lengths, or throws an error or warning. Just like with assertions and `assertthat`, you can test multiple facets of a single object to make sure things are exactly as you've specified. 
---
class:inverse
## Your Turn - Writing Expectations

1. Create a test file with `usethis::use_test("mtcars")`

2. Write a set of expectations that test the `mtcars` data to make sure it has the correct format    
What are the important/essential features of the data?


.center[![:scale 30%](https://www.aliensthetruth.com/images/ufo-abduction.jpg)]
.center[If `mtcars` was abducted, would you notice?]
???
Write a series of tests for the `mtcars` data - if you accidentally overwrote the default mtcars, what would you want to test to make sure it was the real thing?

To do this, you'll have to identify the essential features of the data - if the structure is essentially the same, your functions should work correctly, but if the structure changes in some way, you would need to detect those changes. 

Focus on writing the expectations right now - we can put them into a test block afterwards.

---

## Solutions

```r
data(mtcars)

# correct type
expect_s3_class(mtcars, "data.frame")

# correct dimensions
expect_equivalent(dim(mtcars), c(32, 11))

# correct column names
expect_named(mtcars, c("mpg", "cyl", "disp", "hp", "drat", 
                       "wt", "qsec", "vs", "am", "gear", "carb"))
```
???
First and foremost, we care that it's the right data type - a data.frame, which is an s3 class.

We want mtcars to have the right number of dimensions - you could test these separately with `nrow` and `ncol` if you want. 

Then we want to make sure it has the right names, right? If we refer to the columns by name, we need to make sure they're all there!
---

## Solutions

```r
# hardcore option: use dput() to record mtcars structure
# dput(head(mtcars))
mtcars_head &lt;- structure(
  list(
    mpg = c(21, 21, 22.8, 21.4, 18.7, 18.1), 
    cyl = c(6, 6, 4, 6, 8, 6), 
    disp = c(160, 160, 108, 258, 360, 225), 
    hp = c(110, 110, 93, 110, 175, 105), 
    drat = c(3.9, 3.9, 3.85, 3.08, 3.15, 2.76), 
    wt = c(2.62, 2.875, 2.32, 3.215, 3.44, 3.46), 
    qsec = c(16.46, 17.02, 18.61, 19.44, 17.02, 20.22), 
    vs = c(0, 0, 1, 1, 0, 1), 
    am = c(1, 1, 1, 0, 0, 0), 
    gear = c(4, 4, 4, 3, 3, 3), 
    carb = c(4, 4, 1, 1, 2, 1)), 
  row.names = c("Mazda RX4", "Mazda RX4 Wag", "Datsun 710", 
                "Hornet 4 Drive", "Hornet Sportabout", "Valiant"), 
  class = "data.frame")
expect_equivalent(head(mtcars), mtcars_head)
```
Choose your tests carefully - too specific, and they'll break unnecessarily

???

One way to make sure that the structure is what you want it to be is to use `dput`, which provides the code to generate an R object. You can paste that into the test file, and then test whether the two objects are equivalent. 

In this case, this may not be the *best* option, but it's a useful demonstration.

---

## Solutions

```r
library(checkmate)

test_data_frame(mtcars,
                # correct dimensions 
                nrow = 32, ncol = 11)
```

```
## [1] TRUE
```

```r
test_names(mtcars, 
           must.include = c("mpg", "cyl", "disp", "hp", "drat", 
                            "wt", "qsec", "vs", "am", "gear", "carb"))
```

```
## [1] FALSE
```

???

Some of you may have used checkmate to do this - as with the assertions, you can test things with fewer lines, but you may not get error messages that are as descriptive.

---

## Solutions

Once we have our set of tests, we turn them into a test block:


```r
context("testing mtcars")
data(mtcars)

test_that("mtcars is properly formatted", {
expect_s3_class(mtcars, "data.frame")
expect_equivalent(dim(mtcars), c(32, 11))
expect_named(mtcars, c("mpg", "cyl", "disp", "hp", "drat", 
                       "wt", "qsec", "vs", "am", "gear", "carb"))
})
```
???
Once you have the expectations written, it's fairly easy to fit them into a structured testthat file: just add the expectations to a `test_that` function, add the description, and add the `test_that()` function and context to a file.
---

## Test-Driven Development Workflow

1. Write out the specifications for your function

2. Create your test file `usethis::use_test("foo")`

3. Write tests for your function in `tests/testthat/test_foo.R`

4. Write your function and documentation in `R/foo.R`

5. Test your package: `Ctrl/CMD - Shift - T` or use the Build Menu

.center[![:scale 60%](rstudio-build-interface-tests.png)]

???
If you use the test driven development workflow, you'll follow these steps: 

First, you write out what your function is going to do and what structure it will have. Then you create the test file for the function and fill in your tests - each thing the function is supposed to do will require a test to ensure that function is working correctly.

Once your tests are written, you make the .R file where the actual function will live. You write the function so that it passes the tests, and you document it as you go (since you already wrote out what the function does, this doesn't take long). 

Then, you test your function to make sure it passes the tests. If it doesn't, modify your code (or tests) and rerun the tests until it passes. 
---

## Test-Driven Development Example

Function: `mymean(x, na.rm)`

Important behaviors: 

???
Suppose we want to create a function `mymean` that has parameters x and na.rm. We want this function to issue an error if x is not numeric, and if it is numeric to calculate the mean. If x contains NAs, the function should return NA if na.rm = F, and the mean of x without any NA values if na.rm = T. 
--

- issue a warning when x is not numeric and return NA

--

- return a value equal to `sum(x)/length(x)` when x has no `NA` values 

- return a value equal to `sum(x2)/length(x2)` where `x2 = x[!is.na(x)]` if `na.rm = T`

- return `NA` when x has `NA`s and `na.rm = F`

---

## Test-Driven Development Example

Important behaviors: 

- issue a warning when x is not numeric    
(or if x cannot be coerced to a numeric vector)

```r
x &lt;- letters[1:3]
expect_warning(mymean(x)) # This is the minimal test
expect_warning(mymean(x), # This tests for a specific warning
               "argument is not numeric or logical: returning NA") 

# This tests the value and that a warning is generated
expect_warning(
  expect_true(is.na(mymean(x))) # Function returns NA
)
```

???
Our expectation for non-numeric input is that mymean returns NA and a warning. We could test for a specific warning, or we could test that a warning is issued and leave it at that. Initially, it's probably better to test for the presence of a warning, but as your package matures you may want to check that specific warnings are generated to make sure your function is responding to the right thing.
We also want to test that mymean returns an NA. Note that this will still generate a warning, so we have to actually wrap the two functions.
---

## Test-Driven Development Example

Important behaviors: 
- return a single numeric value equal to `sum(x)/length(x)`


```r
x &lt;- 1:8
y &lt;- c(1:8, NA)

# Test that mean(1:8) returns a numerically correct response
expect_equal(mymean(x), 4.5)

# Test that mean(c(1:8, NA)) equals mean(1:8) when na.rm = T
expect_equal(mymean(y, na.rm = T), mymean(x))
```
???

We can first test that mymean returns a correct response, and then check that it returns the same response when an NA is added and na.rm is true.
---

## Test-Driven Development Example

- return `NA` when `na.rm = F` and `x` has `NA`s

```r
# Can also test to see if a test fails...
expect_failure(
  expect_equal(mymean(c(1:8, NA), na.rm = F), mymean(c(1:8)))
)

# Or test if something returns NA
expect_true(is.na(mymean(c(1:8, NA))))
```
???
We can expect a test to fail, which is a bit strange but useful - we want the values to be different in thsi case. Alternately, we can test to see if the function returns NA, which is a more specific test of the behavior we want.
---
class:inverse
## Your Turn

In your `helloR` package, do the following: 

1. Create the file `R/mymean.R` for your `mymean` function

2. Create a test file for `mymean`: `usethis::use_test("mymean")`

3. Add the expectations in the previous slides to your test file in a test block

4. Write the `mymean` function in `R/mymean.R` Use `roxygen` tags to document the function. 

5. Save your test and code files, build your package (`Ctrl/CMD-Shift-B`), then test your package (`Ctrl/CMD-Shift-T`). Did your function pass?

???
In this yourturn, use the expectations we talked about in the previous slides and implement `mymean` for your package. Use Test-driven development practices - so make the test file first, then write the function. Add documentation to your function, and when you're done, save your files, build your package, and test it. Does your function pass?

---
class:inverse
## Your Turn

Create a new function for your `helloR` package. 

- Function specifications: 
    - name: `sign_root`
    - Parameters n and value, both numeric
        - Function should error if `n` or `value` is not numeric
    - Function should return a number 
        - with the same sign as `value`
        - with magnitude equal to the `n`th root of `value`.

1. Create a test file for this function and write appropriate tests

2. Write the function    
.small[Don't forget to add assertions to your function to check inputs!&lt;br/&gt;The `checkmate` package is your friend!]

3. Does your function pass your tests? How do your tests compare to your neighbor's?
???
This time, I want you to take a function description and write the tests and expectations from scratch. Use the same workflow you used in the previous yourturn. Remember to use assertions in your function! They're helpful to generate good error messages - messages that you can test!

---

## Solution - Tests


```r
context("sign_root")

# - Function specifications:
#   - name: `sign_root`
# - Parameters n and value, both numeric
# - Function should error if `n` or `value` is not numeric
# - Function should return a number
# - with the same sign as `value`
# - with magnitude equal to the `n`th root of `value`.

test_that("sign_root works as expected", {
  expect_error(sign_root(n = "a", value = 2))
  expect_error(sign_root(n = 2, value = "a"))
  expect_equivalent(sign_root(2, 4), 2)
  expect_equivalent(sign_root(2, -4), -2)
  expect_equivalent(sign_root(3, 8), 2)
  expect_equivalent(sign_root(3, -8), -2)
})
```

---

## Solution - Function


```r
#' Take the nth root of a value, but keep the sign intact
#'
#' @param n root to take (n = 2 would correspond to the square root)
#' @param value value to take the root of
#' @return the nth root of abs(value) with the sign of value
#' @export
sign_root &lt;- function(n = 2, value = 4) {
  checkmate::check_number(n, na.ok = T)
  checkmate::check_number(value, na.ok = T)

  abs(value) ^ (1/n) * sign(value)
}
```

---

## Unit tests are...

- Modular (by design)

- Quick to run

- Run in an clean environment&lt;br/&gt;    
    - Set up and Tear down tasks set up the environment for testing
    
- Independent - don't require outside files (traditionally)

---

## Unit tests

&gt; But, we're statisticians. What about the data?

- use tiny test data (`dput()` is helpful)

- read in toy data from files in the test directory

- use data included with the package

- use data included in base R

- download data from elsewhere with a set-up function, delete it with a tear-down function
    - tests will fail without internet or if the site is down


---

## Package Checking

Unit tests test specific functions you've written. Package checks test the whole package, including

- the package has the correct file structure

- the package is installable

- there is a recognized license

- there is an author/maintaner (with contact information)

- documentation exists and is correctly structured

- imports and dependencies are correctly specified

- examples can be run

- and more...

---

## Package Checking

`devtools::check()` or `Ctrl/CMD-Shift-E` will check your package

To submit to CRAN, you must not have any warnings or errors.

Workflow:

- `Ctrl/CMD-Shift-E` (builds documentation, compiles package, runs checks)

- Fix the first problem

- Repeat until there are no more problems

---
class:inverse
## Your Turn

Get your package working with `devtools::check()`. You may have to: 

- add documentation    
(Build menu -&gt; Generate documentation with roxygen, then Ctrl/Cmd-Shift-D)

- add a license: `usethis::use_gpl3_license(name="your name")`

- make sure your tests pass

---
class:middle,center
# Continuous Integration - TravisCI

---

## Motivation

The package build - modify - test cycle can be tedious. 

Sometimes, your package is written in a way that depends on your R environment. 

How do you determine if things will work on a different computer?

--

You test the package build/install process as well!

---

## Continuous Integration

[Travis CI](https://travis-ci.org/) can be used with github to automatically 
1. set up a test environment
2. install all dependencies
3. build your package
4. run all tests
5. (optional) calculate code coverage and upload to a service like [codecov.io](https://codecove.io)

---

## Continuous Integration

Setting up Travis for your R package:

1. [Make an account](https://travis-ci.org/) and link to your GitHub account
2. Switch Travis monitoring on [for the repository of your choice](https://travis-ci.org/account/repositories)
![:scale 60%](switch_travis_repo_on.png)

---

## Continuous Integration

Setting up Travis for your R package:

1. [Make an account](https://travis-ci.org/) and link to your GitHub account
2. Switch Travis monitoring on [for the repository of your choice](https://travis-ci.org/account/repositories)
3. Enable travis for your package: `usethis::use_travis()`

A new build will be triggered when you push changes to your github repository.

---
class:inverse
## Your Turn

1. Set up travis for your helloR repository

2. Try to get your travis build working.


---

## Continuous Integration

&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;Things are going great; why do you ask?! &lt;a href="https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw"&gt;#rstats&lt;/a&gt; 

![:scale 70%](https://pbs.twimg.com/media/CgbixfuVAAAWgZn.jpg)

&lt;/p&gt;&amp;mdash; Julia Silge (@juliasilge) &lt;a href="https://twitter.com/juliasilge/status/722514130968535042?ref_src=twsrc%5Etfw"&gt;April 19, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

---

## Continuous Integration

The last Your Turn resulted in...
![](travis_is_depressing.png)

.center[![:scale 70%](travis_is_depressing2.png)]

---
class:middle,center
# Code Coverage: `covr`

---

## Code Coverage

&gt; How many unit tests are enough? Is everything tested?

- `covr` is a package that will:
    1. build your package in a clean environment (using TravisCI)
    2. run your tests
    3. determine how many times each line was evaluated (through [magic](https://cran.r-project.org/web/packages/covr/vignettes/how_it_works.html))
    4. launch a Shiny app to show you line-by-line coverage reports
    
- 100% coverage is good, but unit tests aren't everything
    - Some lines aren't worth testing
    - Integration testing matters too!


```r
covr::report() # Run a local code coverage report

# Enable codecov.io with your github account
# This requires travis integration, but generates coverage reports 
#   automatically with every change you push to the github repo
usethis::use_coverage(type = "codecov") 
```

---
class:inverse
## Your Turn

1. Run a code coverage report locally for your helloR package using `covr::report()`

2. Stretch goal: Try to get coverage configured using [codecov.io](https://codecov.io/github/srvanderplas/helloR?branch=master)
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
